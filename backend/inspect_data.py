import pandas as pd
import duckdb
import torch
from pathlib import Path

DATA_DIR = Path(__file__).parent / "data/Processed"

def load_parquet_safe(path):
    """Lit un fichier parquet m√™me s'il est corrompu"""
    try:
        return pd.read_parquet(path)
    except Exception as e:
        print(f"‚ö†Ô∏è PyArrow a √©chou√©, utilisation de DuckDB pour : {path}")
        print(f"   Erreur: {e}")
        return duckdb.query(f"SELECT * FROM read_parquet('{path}')").to_df()

def inspect_dataframe(df, name):
    """Affiche toutes les informations sur un DataFrame"""
    if df is None:
        print(f"\n‚ùå {name}: Fichier non trouv√© ou vide")
        return
    
    print(f"\n{'='*80}")
    print(f"üìä {name}")
    print(f"{'='*80}")
    
    # Informations de base
    print(f"\nüìè Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    
    # Colonnes et types
    print(f"\nüìã Colonnes ({len(df.columns)}):")
    print("-" * 80)
    for col in df.columns:
        dtype = df[col].dtype
        non_null = df[col].notna().sum()
        null_count = df[col].isna().sum()
        null_pct = (null_count / len(df)) * 100 if len(df) > 0 else 0
        
        print(f"  ‚Ä¢ {col:30s} | Type: {str(dtype):15s} | Non-null: {non_null:6d} ({100-null_pct:.1f}%)")
    
    # Types de donn√©es par colonne
    print(f"\nüî¢ Types de donn√©es:")
    print("-" * 80)
    type_counts = df.dtypes.value_counts()
    for dtype, count in type_counts.items():
        print(f"  ‚Ä¢ {str(dtype):20s}: {count} colonnes")
    
    # Statistiques descriptives pour les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns
    if len(numeric_cols) > 0:
        print(f"\nüìà Statistiques descriptives (colonnes num√©riques):")
        print("-" * 80)
        print(df[numeric_cols].describe().to_string())
    
    # Valeurs uniques pour les colonnes cat√©gorielles (limit√© √† 20)
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        print(f"\nüè∑Ô∏è  Valeurs uniques (colonnes cat√©gorielles, max 20 valeurs):")
        print("-" * 80)
        for col in categorical_cols[:10]:  # Limiter √† 10 colonnes pour ne pas surcharger
            unique_vals = df[col].unique()
            unique_count = len(unique_vals)
            print(f"\n  ‚Ä¢ {col} ({unique_count} valeurs uniques):")
            if unique_count <= 20:
                for val in unique_vals[:20]:
                    count = (df[col] == val).sum()
                    print(f"      - {val}: {count} fois")
            else:
                print(f"      (Trop de valeurs, affichage des 10 premi√®res)")
                for val in unique_vals[:10]:
                    count = (df[col] == val).sum()
                    print(f"      - {val}: {count} fois")
                print(f"      ... et {unique_count - 10} autres valeurs")
    
    # Aper√ßu des donn√©es
    print(f"\nüëÄ Aper√ßu des donn√©es (5 premi√®res lignes):")
    print("-" * 80)
    print(df.head().to_string())
    
    # Informations sur les valeurs manquantes
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print(f"\n‚ö†Ô∏è  Valeurs manquantes:")
        print("-" * 80)
        for col, count in missing[missing > 0].items():
            pct = (count / len(df)) * 100
            print(f"  ‚Ä¢ {col:30s}: {count:6d} ({pct:.1f}%)")
    else:
        print(f"\n‚úÖ Aucune valeur manquante")

def inspect_embeddings(embeddings, name):
    """Affiche les informations sur les embeddings PyTorch"""
    if embeddings is None:
        print(f"\n‚ùå {name}: Fichier non trouv√© ou vide")
        return
    
    print(f"\n{'='*80}")
    print(f"üî¢ {name}")
    print(f"{'='*80}")
    
    if isinstance(embeddings, dict):
        print(f"\nüì¶ Type: Dictionnaire")
        print(f"üìè Nombre d'√©l√©ments: {len(embeddings)}")
        
        # Afficher les cl√©s (IDs)
        keys = list(embeddings.keys())
        print(f"\nüîë Cl√©s (IDs) - 10 premiers: {keys[:10]}")
        if len(keys) > 10:
            print(f"   ... et {len(keys) - 10} autres IDs")
        
        # Afficher la forme du premier embedding
        if len(keys) > 0:
            first_key = keys[0]
            first_emb = embeddings[first_key]
            if isinstance(first_emb, torch.Tensor):
                print(f"\nüìê Shape du premier embedding (ID={first_key}): {first_emb.shape}")
                print(f"   Type: {first_emb.dtype}")
                print(f"   Device: {first_emb.device}")
            else:
                print(f"\nüìê Premier embedding (ID={first_key}): {type(first_emb)}")
                print(f"   Valeur: {first_emb}")
    
    elif isinstance(embeddings, torch.Tensor):
        print(f"\nüì¶ Type: Tensor PyTorch")
        print(f"üìê Shape: {embeddings.shape}")
        print(f"   Type: {embeddings.dtype}")
        print(f"   Device: {embeddings.device}")
        print(f"   Nombre d'√©l√©ments: {embeddings.numel()}")
        
        # Statistiques
        if embeddings.numel() > 0:
            print(f"\nüìä Statistiques:")
            print(f"   Min: {embeddings.min().item():.4f}")
            print(f"   Max: {embeddings.max().item():.4f}")
            print(f"   Mean: {embeddings.mean().item():.4f}")
            print(f"   Std: {embeddings.std().item():.4f}")
    else:
        print(f"\nüì¶ Type: {type(embeddings)}")
        print(f"   Contenu: {embeddings}")

def main():
    print("üîç INSPECTION DES DONN√âES")
    print("="*80)
    
    # Inspecter les fichiers parquet
    files_to_check = [
        ("jobs.parquet", "Jobs"),
        ("jobs_sample.parquet", "Jobs Sample"),
        ("students.parquet", "Students"),
        ("interactions.parquet", "Interactions")
    ]
    
    for filename, name in files_to_check:
        filepath = DATA_DIR / filename
        if filepath.exists():
            try:
                df = load_parquet_safe(filepath)
                inspect_dataframe(df, name)
            except Exception as e:
                print(f"\n‚ùå Erreur lors de l'inspection de {filename}: {e}")
        else:
            print(f"\n‚ö†Ô∏è  Fichier non trouv√©: {filename}")
    
    # Inspecter les embeddings
    print("\n\n" + "="*80)
    print("üî¢ INSPECTION DES EMBEDDINGS")
    print("="*80)
    
    # Job embeddings
    job_emb_path = DATA_DIR / "job_embeddings.pt"
    if job_emb_path.exists():
        try:
            job_embeddings = torch.load(job_emb_path, map_location='cpu')
            inspect_embeddings(job_embeddings, "Job Embeddings")
        except Exception as e:
            print(f"\n‚ùå Erreur lors du chargement de job_embeddings.pt: {e}")
    else:
        print(f"\n‚ö†Ô∏è  Fichier non trouv√©: job_embeddings.pt")
    
    # User embeddings
    user_emb_path = DATA_DIR / "user_embeddings.pt"
    if user_emb_path.exists():
        try:
            user_embeddings = torch.load(user_emb_path, map_location='cpu')
            inspect_embeddings(user_embeddings, "User Embeddings")
        except Exception as e:
            print(f"\n‚ùå Erreur lors du chargement de user_embeddings.pt: {e}")
    else:
        print(f"\n‚ö†Ô∏è  Fichier non trouv√©: user_embeddings.pt")
    
    print("\n" + "="*80)
    print("‚úÖ Inspection termin√©e")
    print("="*80)

if __name__ == "__main__":
    main()
